{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\"LocalAPILLM\" object has no field \"api_base\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Instantiate the custom LLM with your local API details\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m local_llm \u001b[38;5;241m=\u001b[39m \u001b[43mLocalAPILLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:11434/v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Use the LLM with a prompt\u001b[39;00m\n\u001b[0;32m     38\u001b[0m response \u001b[38;5;241m=\u001b[39m local_llm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m, in \u001b[0;36mLocalAPILLM.__init__\u001b[1;34m(self, api_base, model_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, api_base: \u001b[38;5;28mstr\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_base\u001b[49m \u001b[38;5;241m=\u001b[39m api_base\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\pydantic\\main.py:884\u001b[0m, in \u001b[0;36mBaseModel.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_assignment(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_fields:\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;66;03m# TODO - matching error\u001b[39;00m\n\u001b[1;32m--> 884\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m object has no field \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_fields:\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_extra \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_extra:\n",
      "\u001b[1;31mValueError\u001b[0m: \"LocalAPILLM\" object has no field \"api_base\""
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM\n",
    "import requests\n",
    "from typing import Optional, List\n",
    "\n",
    "class LocalAPILLM(LLM):\n",
    "    def __init__(self, api_base: str, model_name: str):\n",
    "        self.api_base = api_base\n",
    "        self.model_name = model_name\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"local_api\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        # Construct the API request payload\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 1024,         # Adjust as needed\n",
    "            \"temperature\": 0.1,         # Adjust as needed\n",
    "            \"stop\": stop\n",
    "        }\n",
    "\n",
    "        # Send the request to the local model API\n",
    "        response = requests.post(f\"{self.api_base}/generate\", json=payload)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "\n",
    "        # Extract the generated text from the response\n",
    "        return response.json().get(\"generated_text\", \"\")\n",
    "\n",
    "# Instantiate the custom LLM with your local API details\n",
    "local_llm = LocalAPILLM(api_base=\"http://localhost:11434/v1\", model_name=\"llama3\")\n",
    "\n",
    "# Use the LLM with a prompt\n",
    "response = local_llm(\"What is the capital of France?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Artificial intelligence is intelligence shown by machines, focusing on intelligent agents that perceive their environment and achieve their goals.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'llama3.2' did not respond with valid JSON. \n                Please try again. \n                Response: {\n  \"tool\": \"__conversational_response\",\n  \"response\": \"The capital of France is Paris.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\langchain_experimental\\llms\\ollama_functions.py:320\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     parsed_chat_result \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_generation_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 3 column 48 (char 88)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# --- Direct invocation of the Ollama model ---\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Use the invoke method to directly query the model with a specific prompt\u001b[39;00m\n\u001b[0;32m     34\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse to direct invocation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\langchain_experimental\\llms\\ollama_functions.py:322\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     parsed_chat_result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(chat_generation_content)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m did not respond with valid JSON. \u001b[39m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124m        Please try again. \u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124m        Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat_generation_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    326\u001b[0m     )\n\u001b[0;32m    327\u001b[0m called_tool_name \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    328\u001b[0m     parsed_chat_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parsed_chat_result \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    329\u001b[0m )\n\u001b[0;32m    330\u001b[0m called_tool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m    331\u001b[0m     (fn \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m functions \u001b[38;5;28;01mif\u001b[39;00m fn[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m called_tool_name), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    332\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: 'llama3.2' did not respond with valid JSON. \n                Please try again. \n                Response: {\n  \"tool\": \"__conversational_response\",\n  \"response\": \"The capital of France is Paris.\""
     ]
    }
   ],
   "source": [
    "# Import necessary modules from LangChain and Ollama\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize OllamaFunctions with your local Ollama API and model name\n",
    "ollama_llm = OllamaFunctions(api_base=\"http://localhost:11434/v1\", model=\"llama3.2\")\n",
    "\n",
    "# Define a prompt template for summarization\n",
    "summarization_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],  # Variable to be used in the prompt\n",
    "    template=\"Summarize the following text: {text}\"  # Template for the summarization task\n",
    ")\n",
    "\n",
    "# Set up the Summarization Chain\n",
    "summarization_chain = LLMChain(\n",
    "    llm=ollama_llm,  # The LLM used in the chain\n",
    "    prompt=summarization_prompt  # The prompt template to use\n",
    ")\n",
    "\n",
    "# Example document to summarize\n",
    "docs = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\n",
    "Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\n",
    "\"\"\"\n",
    "\n",
    "# Run the summarization chain with the provided document\n",
    "summary = summarization_chain.run(text=docs)\n",
    "print(\"Summary:\", summary)\n",
    "\n",
    "# --- Direct invocation of the Ollama model ---\n",
    "# Use the invoke method to directly query the model with a specific prompt\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = ollama_llm.invoke(prompt)\n",
    "print(\"Response to direct invocation:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/dunzhang/stella_en_400M_v5:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/dunzhang/stella_en_400M_v5:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "please install xformers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the embeddings with the dunzhang/stella_en_400M_v5 model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdunzhang/stella_en_400M_v5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrust_remote_code\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Example text to verify the embeddings\u001b[39;00m\n\u001b[0;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a test sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:92\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43msentence_transformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:306\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    297\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[0;32m    300\u001b[0m     model_name_or_path,\n\u001b[0;32m    301\u001b[0m     token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    305\u001b[0m ):\n\u001b[1;32m--> 306\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[0;32m    319\u001b[0m         model_name_or_path,\n\u001b[0;32m    320\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    327\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    328\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1722\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   1719\u001b[0m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1722\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1724\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(model_name_or_path)\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:76\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[0;32m     73\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     75\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[0;32m     79\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:108\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir, backend, **model_args)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_onnx_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4097\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4091\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[0;32m   4092\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[0;32m   4093\u001b[0m     )\n\u001b[0;32m   4095\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m   4096\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m-> 4097\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[0;32m   4100\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\dunzhang\\stella_en_400M_v5\\2aa5579fcae1c579de199a3866b6e514bbbf5d10\\modeling.py:823\u001b[0m, in \u001b[0;36mNewModel.__init__\u001b[1;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m NewEmbeddings(config)\n\u001b[1;32m--> 823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mNewEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;241m=\u001b[39m NewPooler(config) \u001b[38;5;28;01mif\u001b[39;00m add_pooling_layer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\dunzhang\\stella_en_400M_v5\\2aa5579fcae1c579de199a3866b6e514bbbf5d10\\modeling.py:696\u001b[0m, in \u001b[0;36mNewEncoder.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mNewLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\dunzhang\\stella_en_400M_v5\\2aa5579fcae1c579de199a3866b6e514bbbf5d10\\modeling.py:630\u001b[0m, in \u001b[0;36mNewLayer.__init__\u001b[1;34m(self, config, pack_qkv, use_memory_efficient_attention, attn_implementation)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    629\u001b[0m     use_memory_efficient_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m \u001b[43mNEW_ATTENTION_CLASSES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpack_qkv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_qkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_memory_efficient_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_memory_efficient_attention\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m NewGatedMLP(config)\n\u001b[0;32m    635\u001b[0m ln_class \u001b[38;5;241m=\u001b[39m LAYER_NORM[config\u001b[38;5;241m.\u001b[39mlayer_norm_type]\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\dunzhang\\stella_en_400M_v5\\2aa5579fcae1c579de199a3866b6e514bbbf5d10\\modeling.py:451\u001b[0m, in \u001b[0;36mNewAttention.__init__\u001b[1;34m(self, config, pack_qkv, use_memory_efficient_attention)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_efficient_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m xops \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m xops\u001b[38;5;241m.\u001b[39mmemory_efficient_attention\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_memory_efficient_attention:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_efficient_attention \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease install xformers\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39munpad_inputs:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_memory_efficient_attention, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munpad only with xformers\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: please install xformers"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize the embeddings with the dunzhang/stella_en_400M_v5 model\n",
    "embeddings = HuggingFaceEmbeddings(model_name='dunzhang/stella_en_400M_v5', model_kwargs={'trust_remote_code': True})\n",
    "\n",
    "# Example text to verify the embeddings\n",
    "text = [\"Hello, how are you?\", \"This is a test sentence.\"]\n",
    "\n",
    "# Embed the documents and print the embeddings\n",
    "embedding_vectors = embeddings.embed_documents(text)\n",
    "\n",
    "# Print out the shape and type to verify if embeddings are generated\n",
    "print(type(embedding_vectors))  # Should be a list of lists (or numpy array if you use one)\n",
    "print(len(embedding_vectors))   # Number of sentences processed\n",
    "print(len(embedding_vectors[0]))  # Length of embedding vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country population year\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the CSV and get column names\n",
    "def get_column_names_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return set(df.columns.str.lower())  # Standardize to lowercase for consistent matching\n",
    "\n",
    "# Dynamically match query terms to column names\n",
    "def interpret_question(query, file_path):\n",
    "    # Get the column names from the CSV\n",
    "    column_names = get_column_names_from_csv(file_path)\n",
    "    \n",
    "    # Parse the question with spaCy\n",
    "    doc = nlp(query.lower())  # Convert query to lowercase for case-insensitive matching\n",
    "    \n",
    "    # Collect relevant keywords\n",
    "    keywords = set()\n",
    "    \n",
    "    # Function to match terms dynamically with column names\n",
    "    def match_term_to_column(term):\n",
    "        # First, check for an exact match or a close match\n",
    "        for col in column_names:\n",
    "            if term == col:\n",
    "                return col\n",
    "        \n",
    "        # Check for partial match within column names\n",
    "        for col in column_names:\n",
    "            if term in col or col in term:  # Allows \"gdp\" to match \"gdp_per_capita\"\n",
    "                return col\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Match entities and noun chunks dynamically to column names\n",
    "    for ent in doc.ents:\n",
    "        matched_col = match_term_to_column(ent.text)\n",
    "        if matched_col:\n",
    "            keywords.add(matched_col)\n",
    "    \n",
    "    for chunk in doc.noun_chunks:\n",
    "        matched_col = match_term_to_column(chunk.text)\n",
    "        if matched_col:\n",
    "            keywords.add(matched_col)\n",
    "    \n",
    "    # Add individual nouns and important terms if not already matched\n",
    "    if not keywords:\n",
    "        for token in doc:\n",
    "            if token.pos_ in {\"NOUN\", \"PROPN\", \"NUM\"} and not token.is_stop:\n",
    "                matched_col = match_term_to_column(token.text)\n",
    "                if matched_col:\n",
    "                    keywords.add(matched_col)\n",
    "    \n",
    "    # Ensure time-based terms like \"year\" are included if present in column names\n",
    "    time_terms = [\"year\", \"date\", \"time\"]\n",
    "    for term in time_terms:\n",
    "        matched_col = match_term_to_column(term)\n",
    "        if matched_col:\n",
    "            keywords.add(matched_col)\n",
    "    \n",
    "    # Join keywords into a refined query\n",
    "    refined_query = \" \".join(keywords)\n",
    "    return refined_query\n",
    "\n",
    "# Example usage\n",
    "file_path = \"data/world_bank_dataset.csv\"\n",
    "query = \"Which country has the highest population and low gdp in the year 2010?\"\n",
    "print(interpret_question(query, file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load spaCy model for named entity recognition (NER)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\gensim\\matutils.py:1034\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 \u001b[38;5;241m&\u001b[39m set2)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogsumexp\u001b[39m(x):\n",
      "File \u001b[1;32mc:\\Users\\TEJA9\\Desktop\\ChatCSV-Llama2-Chatbot-main\\.venv\\Lib\\site-packages\\gensim\\_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load spaCy model for named entity recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load a pre-trained Word2Vec model (e.g., Google News embeddings or other pre-trained model)\n",
    "# You can download the Google Word2Vec model from https://code.google.com/archive/p/word2vec/\n",
    "# For the sake of this example, we will assume you have a word2vec model file loaded.\n",
    "\n",
    "# Assuming 'word_vectors' is the pre-trained model (for example, Google's word2vec)\n",
    "# word_vectors = KeyedVectors.load_word2vec_format('path_to_pretrained_model', binary=True)\n",
    "\n",
    "# Load CSV and get column names\n",
    "def get_column_names_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return list(df.columns)\n",
    "\n",
    "# Extract terms dynamically from query using NER and spaCy\n",
    "def interpret_question(query, file_path):\n",
    "    column_names = get_column_names_from_csv(file_path)\n",
    "    doc = nlp(query.lower())  # Process the query text\n",
    "    \n",
    "    # Extracting entities and important words from the query\n",
    "    keywords = set()\n",
    "    for ent in doc.ents:\n",
    "        keywords.add(ent.text)\n",
    "\n",
    "    # Also extract noun chunks, as they could contain important terms like \"gdp\" or \"population\"\n",
    "    for chunk in doc.noun_chunks:\n",
    "        keywords.add(chunk.text)\n",
    "\n",
    "    # If no matches found, consider individual noun tokens\n",
    "    if not keywords:\n",
    "        for token in doc:\n",
    "            if token.pos_ in {\"NOUN\", \"PROPN\", \"NUM\"} and not token.is_stop:\n",
    "                keywords.add(token.text)\n",
    "    \n",
    "    # We need to convert both the query and column names to vectors for comparison\n",
    "    query_vector = None\n",
    "    for word in keywords:\n",
    "        try:\n",
    "            word_vector = word_vectors[word]\n",
    "            if query_vector is None:\n",
    "                query_vector = word_vector\n",
    "            else:\n",
    "                query_vector += word_vector  # Summing up word vectors\n",
    "        except KeyError:\n",
    "            continue  # Skip words that don't have embeddings\n",
    "    \n",
    "    # If no embeddings were found for the query, return an empty result\n",
    "    if query_vector is None:\n",
    "        return \"No match found in query.\"\n",
    "\n",
    "    # Compare the vectorized query with column names to find the most relevant column\n",
    "    max_similarity = -1\n",
    "    best_match_column = None\n",
    "    for column in column_names:\n",
    "        column_vector = None\n",
    "        for word in column.split():\n",
    "            try:\n",
    "                word_vector = word_vectors[word]\n",
    "                if column_vector is None:\n",
    "                    column_vector = word_vector\n",
    "                else:\n",
    "                    column_vector += word_vector\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        if column_vector is not None:\n",
    "            similarity = cosine_similarity([query_vector], [column_vector])[0][0]\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                best_match_column = column\n",
    "    \n",
    "    return best_match_column\n",
    "\n",
    "# Example usage\n",
    "file_path = \"data/world_bank_dataset.csv\"\n",
    "query = \"Which country has the highest population and low gdp in the year 2010?\"\n",
    "result = interpret_question(query, file_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Columns: ['population', 'year', 'year', 'country']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "\n",
    "# Load spaCy model for named entity recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load CSV and get column names\n",
    "def get_column_names_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Normalize column names by removing special characters and converting to lowercase\n",
    "    column_names = [re.sub(r'[^\\w\\s]', '', col.lower()) for col in df.columns]\n",
    "    return column_names\n",
    "\n",
    "# Extract relevant query terms using spaCy\n",
    "def extract_query_terms(query):\n",
    "    doc = nlp(query.lower())  # Lowercase for case-insensitive matching\n",
    "    \n",
    "    # Extract entities and noun chunks\n",
    "    keywords = []\n",
    "    for ent in doc.ents:\n",
    "        keywords.append(ent.text)\n",
    "    \n",
    "    # Extract noun chunks (like 'highest population', 'low gdp')\n",
    "    for chunk in doc.noun_chunks:\n",
    "        keywords.append(chunk.text)\n",
    "    \n",
    "    # Remove duplicates and return the list of query terms\n",
    "    return list(set(keywords))\n",
    "\n",
    "# Fuzzy match the query terms with column names\n",
    "def match_terms_to_columns(query_terms, column_names, threshold=60):\n",
    "    matched_columns = []\n",
    "    for term in query_terms:\n",
    "        # Perform fuzzy matching for each term\n",
    "        matches = process.extractOne(term, column_names, score_cutoff=threshold)\n",
    "        if matches:\n",
    "            matched_columns.append(matches[0])\n",
    "    return matched_columns\n",
    "\n",
    "# Main function to interpret the question and match with columns\n",
    "def interpret_question(query, file_path):\n",
    "    # Get the column names from the CSV file\n",
    "    column_names = get_column_names_from_csv(file_path)\n",
    "    \n",
    "    # Extract relevant terms from the query\n",
    "    query_terms = extract_query_terms(query)\n",
    "    \n",
    "    # Perform fuzzy matching for the extracted query terms\n",
    "    matched_columns = match_terms_to_columns(query_terms, column_names)\n",
    "    \n",
    "    return matched_columns\n",
    "\n",
    "# Example usage\n",
    "file_path = \"data/world_bank_dataset.csv\"  # Path to your dataset\n",
    "query = \"Which country has the highest population and low gdp in the year 2010?\"\n",
    "\n",
    "# Get matched columns\n",
    "matched_columns = interpret_question(query, file_path)\n",
    "print(\"Matched Columns:\", matched_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdp (usd) country population year\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the CSV and get column names\n",
    "def get_column_names_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return set(df.columns.str.lower())  # Standardize to lowercase for consistent matching\n",
    "\n",
    "# Dynamically match query terms to column names\n",
    "def interpret_question(query, file_path):\n",
    "    # Get the column names from the CSV\n",
    "    column_names = get_column_names_from_csv(file_path)\n",
    "    \n",
    "    # Abbreviation mappings to expand terms dynamically\n",
    "    abbreviation_mapping = {\n",
    "        \"gdp\": \"gross domestic product\",\n",
    "        \"pop\": \"population\",\n",
    "        # Add other abbreviations as needed\n",
    "    }\n",
    "    \n",
    "    # Parse the question with spaCy\n",
    "    doc = nlp(query.lower())  # Convert query to lowercase for case-insensitive matching\n",
    "    \n",
    "    # Collect relevant keywords\n",
    "    keywords = set()\n",
    "    \n",
    "    # Function to find closest matching column names for a term\n",
    "    def find_closest_column_name(term):\n",
    "        # Try finding the closest exact or partial match\n",
    "        matches = get_close_matches(term, column_names, n=1, cutoff=0.3)\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "        \n",
    "        # If no close match, attempt a substring search in column names\n",
    "        for col in column_names:\n",
    "            if term in col:\n",
    "                return col\n",
    "        \n",
    "        # If still no match, check the abbreviation mapping\n",
    "        if term in abbreviation_mapping:\n",
    "            expanded_term = abbreviation_mapping[term]\n",
    "            return find_closest_column_name(expanded_term)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Match entities and noun phrases to column names dynamically\n",
    "    for ent in doc.ents:\n",
    "        closest_match = find_closest_column_name(ent.text)\n",
    "        if closest_match:\n",
    "            keywords.add(closest_match)\n",
    "    \n",
    "    for chunk in doc.noun_chunks:\n",
    "        closest_match = find_closest_column_name(chunk.text)\n",
    "        if closest_match:\n",
    "            keywords.add(closest_match)\n",
    "    \n",
    "    # Extract additional nouns if no phrases or entities are matched\n",
    "    if not keywords:\n",
    "        for token in doc:\n",
    "            if token.pos_ in {\"NOUN\", \"PROPN\", \"NUM\"} and not token.is_stop:\n",
    "                closest_match = find_closest_column_name(token.text)\n",
    "                if closest_match:\n",
    "                    keywords.add(closest_match)\n",
    "    \n",
    "    # Ensure essential keywords (e.g., time indicators) are added if they exist in column names\n",
    "    essential_terms = [\"year\", \"date\", \"time\"]\n",
    "    for term in essential_terms:\n",
    "        match = find_closest_column_name(term)\n",
    "        if match:\n",
    "            keywords.add(match)\n",
    "    \n",
    "    # Join keywords into a refined query\n",
    "    refined_query = \" \".join(keywords)\n",
    "    return refined_query\n",
    "\n",
    "# Example usage\n",
    "file_path = \"data/world_bank_dataset.csv\"\n",
    "query = \"Which country has the highest population and low gdp in the year 2010?\"\n",
    "print(interpret_question(query, file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working solutions for refined query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdp (usd) country population year\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the CSV and get column names\n",
    "def get_column_names_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return set(df.columns.str.lower())  # Standardize to lowercase for consistent matching\n",
    "\n",
    "# Function to extract entities based on syntactic dependencies\n",
    "def extract_entities_with_dependencies(doc):\n",
    "    entities = []\n",
    "    for token in doc:\n",
    "        # We collect tokens with certain syntactic dependencies\n",
    "        if token.dep_ in {\"nsubj\", \"dobj\", \"prep\", \"amod\"}:  # Subject, object, prepositions, adjectives\n",
    "            entities.append(token.text)\n",
    "    return entities\n",
    "\n",
    "# Dynamically match query terms to column names\n",
    "def interpret_question(query, file_path):\n",
    "    # Get the column names from the CSV\n",
    "    column_names = get_column_names_from_csv(file_path)\n",
    "    \n",
    "    # Parse the question with spaCy\n",
    "    doc = nlp(query.lower())  # Convert query to lowercase for case-insensitive matching\n",
    "    \n",
    "    # Collect relevant keywords based on entity extraction and dependencies\n",
    "    keywords = set()\n",
    "    \n",
    "    # Extract entities using dependencies\n",
    "    dependency_entities = extract_entities_with_dependencies(doc)\n",
    "    for dep_entity in dependency_entities:\n",
    "        closest_match = find_closest_column_name(dep_entity, column_names)\n",
    "        if closest_match:\n",
    "            keywords.add(closest_match)\n",
    "    \n",
    "    # Match named entities to column names dynamically\n",
    "    for ent in doc.ents:\n",
    "        closest_match = find_closest_column_name(ent.text, column_names)\n",
    "        if closest_match:\n",
    "            keywords.add(closest_match)\n",
    "    \n",
    "    # Match noun chunks to column names dynamically\n",
    "    for chunk in doc.noun_chunks:\n",
    "        closest_match = find_closest_column_name(chunk.text, column_names)\n",
    "        if closest_match:\n",
    "            keywords.add(closest_match)\n",
    "    \n",
    "    # Extract additional nouns if no phrases or entities are matched\n",
    "    if not keywords:\n",
    "        for token in doc:\n",
    "            if token.pos_ in {\"NOUN\", \"PROPN\", \"NUM\"} and not token.is_stop:\n",
    "                closest_match = find_closest_column_name(token.text, column_names)\n",
    "                if closest_match:\n",
    "                    keywords.add(closest_match)\n",
    "    \n",
    "    # Ensure essential keywords (e.g., time indicators) are added if they exist in column names\n",
    "    essential_terms = [\"year\", \"date\", \"time\"]\n",
    "    for term in essential_terms:\n",
    "        match = find_closest_column_name(term, column_names)\n",
    "        if match:\n",
    "            keywords.add(match)\n",
    "    \n",
    "    # Join keywords into a refined query\n",
    "    refined_query = \" \".join(keywords)\n",
    "    return refined_query\n",
    "\n",
    "# Function to find the closest column name from the CSV columns\n",
    "def find_closest_column_name(term, column_names):\n",
    "    # Try finding the closest exact or partial match\n",
    "    matches = get_close_matches(term, column_names, n=1, cutoff=0.3)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    \n",
    "    # If no close match, attempt a substring search in column names\n",
    "    for col in column_names:\n",
    "        if term in col:\n",
    "            return col\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "file_path = \"data/world_bank_dataset.csv\"\n",
    "query = \"Which country has the highest population and low gdp in the year 2010?\"\n",
    "print(interpret_question(query, file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted relevant terms: {'country', '2010', 'gdp', 'population'}\n",
      "Term 'country' matched to column 'Country'\n",
      "Term '2010' matched to column 'Year'\n",
      "Term 'gdp' matched to column 'GDP (USD)'\n",
      "Term 'population' matched to column 'Population'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "\n",
    "# Load pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load spaCy model for NLP processing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load CSV column names (example)\n",
    "column_names = ['GDP (USD)', 'Population', 'Country', 'Year']\n",
    "\n",
    "# Function to extract relevant terms dynamically from the query\n",
    "def extract_relevant_terms(query):\n",
    "    doc = nlp(query.lower())  # Process the query text with spaCy\n",
    "    terms = set()\n",
    "\n",
    "    # Extract entities and key tokens\n",
    "    for ent in doc.ents:\n",
    "        terms.add(ent.text)\n",
    "    \n",
    "    # Also, extract important nouns or keywords that are not entities\n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"NOUN\", \"PROPN\", \"NUM\"} and not token.is_stop:\n",
    "            terms.add(token.text)\n",
    "\n",
    "    return terms\n",
    "\n",
    "# Function to find the best matching column using semantic similarity\n",
    "def find_best_column_match(query, column_names):\n",
    "    # Encode the query and column names to embeddings\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    column_embeddings = model.encode(column_names, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarities between the query and column names\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, column_embeddings)\n",
    "\n",
    "    # Find best match for the query\n",
    "    best_match_idx = cosine_scores.argmax()\n",
    "    return column_names[best_match_idx]\n",
    "\n",
    "# Example query\n",
    "query = \"What country has the highest population and lowest GDP in 2010?\"\n",
    "\n",
    "# Extract relevant terms from the query\n",
    "relevant_terms = extract_relevant_terms(query)\n",
    "\n",
    "# Find best matches for each term in the column names\n",
    "matches = {}\n",
    "for term in relevant_terms:\n",
    "    best_match = find_best_column_match(term, column_names)\n",
    "    matches[term] = best_match\n",
    "\n",
    "# Output the extracted terms and their matched columns\n",
    "print(f\"Extracted relevant terms: {relevant_terms}\")\n",
    "for term, match in matches.items():\n",
    "    print(f\"Term '{term}' matched to column '{match}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Query: lowest country population\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy model and Sentence Transformer model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load CSV and get column names\n",
    "def get_column_names_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df.columns.str.lower().tolist()  # Standardize to lowercase\n",
    "\n",
    "# Function to process the query and extract relevant terms dynamically\n",
    "def process_query(query):\n",
    "    doc = nlp(query.lower())  # Process query with spaCy\n",
    "    terms = set()\n",
    "    comparison_action = None\n",
    "    metrics = set()\n",
    "\n",
    "    # Extract named entities (countries, years, etc.)\n",
    "    for ent in doc.ents:\n",
    "        terms.add(ent.text)\n",
    "\n",
    "    # Detect comparison actions (highest, lowest, etc.) using dependency parsing\n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"amod\", \"superlative\", \"comparative\"}:  # Capturing adjectives like highest, lowest, more, etc.\n",
    "            comparison_action = token.text\n",
    "\n",
    "    # Extract relevant metrics (e.g., GDP, population)\n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"NOUN\", \"PROPN\", \"NUM\"} and not token.is_stop:\n",
    "            metrics.add(token.text)\n",
    "\n",
    "    return terms, comparison_action, metrics\n",
    "\n",
    "# Function to refine the query dynamically\n",
    "def refine_query(query, file_path):\n",
    "    column_names = get_column_names_from_csv(file_path)\n",
    "    relevant_terms, comparison_action, relevant_metrics = process_query(query)\n",
    "    \n",
    "    # Remove any terms that are not found in the columns\n",
    "    refined_terms = [term for term in relevant_terms if term in column_names]\n",
    "    refined_metrics = [metric for metric in relevant_metrics if metric in column_names]\n",
    "\n",
    "    # If a comparison action is found, include it in the refined query\n",
    "    if comparison_action:\n",
    "        refined_terms.append(comparison_action)\n",
    "\n",
    "    # Include relevant metrics in the refined query\n",
    "    refined_terms.extend(refined_metrics)\n",
    "\n",
    "    # Return a refined query based on dynamic extraction\n",
    "    return \" \".join(refined_terms)\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"data/world_bank_dataset.csv\"  # Update with your file path\n",
    "query = \"What country has the highest population and lowest GDP in 2010?\"\n",
    "\n",
    "# Refine the query based on dynamic extraction\n",
    "refined_query = refine_query(query, file_path)\n",
    "print(\"Refined Query:\", refined_query)\n",
    "\n",
    "# Perform document-specific retrieval based on the refined query\n",
    "# docs = docsearch.similarity_search(refined_query, k=5)  # You would use this for actual search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_code_from_response(response):\n",
    "    \"\"\"\n",
    "    This function uses regex to extract code block between triple backticks from the LLM response.\n",
    "    \"\"\"\n",
    "    code_pattern = re.compile(r'```python(.*?)```', re.DOTALL)\n",
    "    match = code_pattern.search(response)\n",
    "    if match:\n",
    "        python_code= match.group(1).strip()\n",
    "        # Remove any line that loads the DataFrame (i.e., pd.read_csv)\n",
    "        #python_code = re.sub(r\"pd\\.read_csv\\([^\\)]*\\)\\s*\", \"\", python_code)\n",
    "        #python_code = re.sub(r\"^(\\s*pd\\.read_csv\\([^\\)]*\\))\", r\"# \\1\", python_code, flags=re.MULTILINE)\n",
    "         # Add a '#' to the beginning of lines with pd.read_csv to comment them out\n",
    "        # This regex will match the line starting with any amount of spaces followed by pd.read_csv\n",
    "        # Remove any lines that contain pd.read_csv\n",
    "        \n",
    "        python_code = re.sub(r\"^\\s*df\\s*=\\s*pd\\.read_csv\\([^\\)]*\\)\\s*$\", \"\", python_code, flags=re.MULTILINE)\n",
    "        print(python_code)\n",
    "\n",
    "        return python_code\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_python_code(python_code):\n",
    "    \"\"\"\n",
    "    This function takes the extracted Python code and executes it in the current environment.\n",
    "    It replaces any placeholders like 'your_data_file.csv' with the actual file path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Replace the placeholder 'your_data_file.csv' with the actual file path\n",
    "        #python_code = re.sub(r\"pd\\.read_csv\\([\\'\\\"]([^\\'\\\"]+)[\\'\\\"]\\)\", \n",
    "        #                     lambda match: f\"pd.read_csv('{file_path}')\", python_code)\n",
    "\n",
    "        # Execute the extracted Python code\n",
    "        exec(python_code)  # Execute the Python code generated by the model\n",
    "        print(\"Code executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing code: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The country with the least life expectancy in the year 2010 is: Argentina\n",
      "Code executed successfully.\n"
     ]
    }
   ],
   "source": [
    "python_code= \"\"\"import pandas as pd\n",
    "\n",
    "# Load your data into a DataFrame\n",
    "df = pd.read_csv('data/world_bank_dataset.csv')\n",
    "\n",
    "# Filter the DataFrame to include only the year 2010\n",
    "df_2010 = df[df['Year'] == 2010]\n",
    "\n",
    "# Find the country with the least life expectancy\n",
    "country_with_least_life_expectancy = df_2010[df_2010['Life Expectancy'] == df_2010['Life Expectancy'].min()]['Country'].values[0]       \n",
    "\n",
    "# Print the result\n",
    "print(f\"The country with the least life expectancy in the year 2010 is: {country_with_least_life_expectancy}\")\"\"\"\n",
    "\n",
    "execute_python_code(python_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "\n",
      "# Load your data into a DataFrame\n",
      "# df = # Filter the DataFrame to include only the year 2010\n",
      "df_2010 = df[df['Year'] == 2010]\n",
      "\n",
      "# Find the country with the least life expectancy\n",
      "country_with_least_life_expectancy = df_2010[df_2010['Life Expectancy'] == df_2010['Life Expectancy'].min()]['Country'].values[0]       \n",
      "\n",
      "# Print the result\n",
      "print(f\"The country with the least life expectancy in the year 2010 is: {country_with_least_life_expectancy}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\n\\n# Load your data into a DataFrame\\n# df = # Filter the DataFrame to include only the year 2010\\ndf_2010 = df[df[\\'Year\\'] == 2010]\\n\\n# Find the country with the least life expectancy\\ncountry_with_least_life_expectancy = df_2010[df_2010[\\'Life Expectancy\\'] == df_2010[\\'Life Expectancy\\'].min()][\\'Country\\'].values[0]       \\n\\n# Print the result\\nprint(f\"The country with the least life expectancy in the year 2010 is: {country_with_least_life_expectancy}\")'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = \"\"\"```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data into a DataFrame\n",
    "# df = pd.read_csv('your_data_file.csv')\n",
    "\n",
    "# Filter the DataFrame to include only the year 2010\n",
    "df_2010 = df[df['Year'] == 2010]\n",
    "\n",
    "# Find the country with the least life expectancy\n",
    "country_with_least_life_expectancy = df_2010[df_2010['Life Expectancy'] == df_2010['Life Expectancy'].min()]['Country'].values[0]       \n",
    "\n",
    "# Print the result\n",
    "print(f\"The country with the least life expectancy in the year 2010 is: {country_with_least_life_expectancy}\")\n",
    "```\"\"\"\n",
    "extract_code_from_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
