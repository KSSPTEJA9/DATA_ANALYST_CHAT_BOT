import re
from llama_cpp import Llama
import pandas as pd

# Initialize the LLaMA model
llm = Llama(
    model_path="./Qwen2.5.1-Coder-7B-Instruct-Q5_K_M.gguf",
    max_tokens=8192,
    temperature=0.1,
    n_threads=8,
    n_ctx=2048
)



file_path = "data/world_bank_dataset.csv"  # Replace with your file path
df = pd.read_csv(file_path) 
# Extract column names from the DataFrame
columns = df.columns.tolist()

# Function to generate a generalized prompt for the task
def generate_prompt(task_description):
    prompt = f"""
You are an expert data analyst. Your task is to write Python code that solves data analysis problems based on the given description.

 Try to understand the question. I had data load in df and now i need to find

Task: {task_description}

and I have these columns in my data  

Available columns in the dataset: {', '.join(columns)}

Please generate the Python code that solves this task using the pandas library. If the task requires filtering a DataFrame, make sure the filtering criteria are clearly mentioned in the code. If the task involves calculations like averages, sums, or groupings, include the corresponding code for those operations.

Be sure to include any necessary imports (like pandas) and explain the steps in the code comments.

The Python code should be concise and clear.

"""
    return prompt


# Function to extract Python code from the LLM response
def extract_code_from_response(response):
    """
    This function uses regex to extract code block between triple backticks from the LLM response.
    """
    code_pattern = re.compile(r'```python(.*?)```', re.DOTALL)
    match = code_pattern.search(response)
    if match:
        python_code= match.group(1).strip()
        # Remove any line that loads the DataFrame (i.e., pd.read_csv)
        python_code = re.sub(r"pd\.read_csv\([^\)]*\)\s*", "", python_code)

        return python_code
    
    else:
        return None
    

# Function to execute the Python code
def execute_python_code(python_code):
    """
    This function takes the extracted Python code and executes it in the current environment.
    It replaces any placeholders like 'your_data_file.csv' with the actual file path.
    """
    try:
        # Replace the placeholder 'your_data_file.csv' with the actual file path
        #python_code = re.sub(r"pd\.read_csv\([\'\"]([^\'\"]+)[\'\"]\)", 
        #                     lambda match: f"pd.read_csv('{file_path}')", python_code)

            # Redirect stdout to capture printed output
        import io
        import sys
        captured_output = io.StringIO()
        sys.stdout = captured_output

        # Execute the extracted Python code
        exec(python_code)  # Execute the Python code generated by the model
        sys.stdout = sys.__stdout__
        print("Code executed successfully.")
        return captured_output.getvalue()
        
    except Exception as e:
        print(f"Error executing code: {e}")


# Chat loop with memory
def chat_with_llm():
    print("Welcome to the LLM Chat with memory! Type 'quit' or 'exit' to end the chat.")

    # Initialize conversation memory
    conversation_history = []

    while True:
        # Get user input
        user_input = input("You: ")

        # Check for exit condition
        if user_input.lower() in ["quit", "exit"]:
            print("Ending the chat. Goodbye!")
            break

        # Add the user input to conversation history
        #conversation_history.append(f"You: {user_input}")

        # Create a single context from the conversation history (joining the last 10 exchanges)
        #context = "\n".join(conversation_history[-10:])  # Limiting to the last 10 exchanges
        context =''

        # Generate a task-specific prompt from the user input
        task_prompt = generate_prompt(user_input)

        # Combine context and the task-specific prompt for LLaMA to generate a code solution
        combined_prompt = f"{task_prompt}\nLLM:"

        # Generate a response from the LLM with conversation history as context
        try:
            response = llm(combined_prompt, max_tokens=8192, stop=["\nYou:"])  # Adjust stop tokens if needed
            
            # Initialize response text and check if it is complete
            llm_response = response['choices'][0]['text'].strip()

            # Check if the response was cut off and continue generating if needed
            # while response['choices'][0].get('finish_reason') == 'length':
            #     # If the response was cut off, continue generating with the last response
            #     response = llm(f"{combined_prompt}\n{llm_response} ...", max_tokens=8192, stop=["\nYou:"])
            #     llm_response += response['choices'][0]['text'].strip()

        except Exception as e:
            # Handle any errors in generation
            llm_response = f"Error: {str(e)}"
        
        # Add the LLM response to the conversation history
        conversation_history.append(f"LLM: {llm_response}")

        # Print the response (the Python code)
        print("LLM:", llm_response)

        # # If the response contains valid Python code, try to execute it
        # if "import pandas" in llm_response and "df" in llm_response:  # Check if it's likely valid code
        #     try:
        #         exec(llm_response)  # Execute the Python code generated by the model
        #         print("Code executed successfully.")
        #     except Exception as e:
        #         print(f"Error executing code: {e}")
        # else:
        #     print("No executable code generated.")

        # Extract the Python code from the LLM response
        python_code = extract_code_from_response(llm_response)

        

        # If the response contains valid Python code, try to execute it
        if python_code:
            print("Executing the extracted code...")
            answer = execute_python_code(python_code)
            print(answer)

        else:
            print("No executable code generated.")

        

# Start the chat
chat_with_llm()
